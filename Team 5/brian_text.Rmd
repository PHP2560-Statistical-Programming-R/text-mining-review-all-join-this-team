---
title: "NYTimes questions"
author: "Brian Gilbert"
date: "October 18, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(suppressWarnings(library(RTextTools)))
suppressMessages(suppressWarnings(library(dplyr)))
suppressMessages(suppressWarnings(library(tidytext)))
suppressMessages(suppressWarnings(library(ggplot2)))
suppressMessages(suppressWarnings(library(caTools)))
suppressMessages(suppressWarnings(library(stringr)))
suppressMessages(suppressWarnings(library(rebus)))
suppressMessages(suppressWarnings(library(foreign)))
suppressMessages(suppressWarnings(library(nnet)))
set.seed(23)
```

## Comparing NRC and Bing

First we load the data and put it in a suitable format for sentiment analysis.

```{r}
data("NYTimes")
NYTimes$Title = as.character(NYTimes$Title)
words = unnest_tokens(NYTimes, word, Title)
nrc.sents = inner_join(words, get_sentiments("nrc"), by="word")
bing.sents = inner_join(words, get_sentiments("bing"), by="word")
```

Now let's examine matches in the text for each dictionary. Note that the following analyses count the same word multiple times if it appears in multiple headlines, which makes sense if we want to see how the dictionaries are interacting with our specific dataset.

```{r}
bing.words <- bing.sents %>% select(word)
nrc.words <- nrc.sents %>% filter(sentiment %in% c("positive", "negative")) %>% select(word)
#The above procedure is done to ensure that each token is counted only once. As described below, 
#NRC gives a positive/negative value exactly once for each word.
dim(bing.words)[1]
dim(nrc.words)[1]
```

The NRC dictionary has significantly more matches in the NYTimes data set. How do the words matched by each dataset compare with respect to length?

```{r}
nrc.lengths <- apply(nrc.words, 1, nchar)
bing.lengths <- apply(nrc.words, 1, nchar)
df <- data.frame(length=c(nrc.lengths, bing.lengths), dict = c(rep("NRC", length(nrc.words)), rep("Bing", length(bing.words))))
p <- ggplot(df, aes(x=factor(dict),y=length,fill=factor(dict)))+
  geom_boxplot() + labs(title="Word Lengths by Dictionary") +facet_wrap(~dict)
p
```

The distributions appear identical.

Again note that the two dictionaries provide different information corresponding to each word: NRC gives multiple specific sentiments along with a positive/negative characterization, while Bing gives just a binary positive/negative characterization. We may investigate whether these two dictionaries give consistent information by restricting NRC's responses to positive/negative and looking at words that match both dictionaries.

```{r}
nrc.binary <- filter(nrc.sents, sentiment %in% c("positive", "negative"))
compare <- inner_join(bing.sents, nrc.binary, by=c("word", "Article_ID"), suffix = c(".bing", ".nrc"))
mean(compare$sentiment.bing==compare$sentiment.nrc)
```

The two dictionaries' sentiments agree 98 percent of the time, but not always. Where do they differ?

```{r}
compare %>% filter(sentiment.bing != sentiment.nrc) %>% 
  select(word, sentiment.bing, sentiment.nrc) %>% 
  group_by(word) %>%
  mutate("Occurences" = n()) %>%
  slice(1) %>%
  ungroup()
```

Interestingly, where the dictionaries differ, Bing seems more likely to return "positive."

Apparently, the most important discrepancy in this dataset is the word "tough." Let's look at the headlines that contain the word "tough."

```{r}
ids = bing.sents %>% 
  filter(word == "tough") %>%
  select(Article_ID) 
NYTimes %>%
  filter(Article_ID %in% unlist(ids)) %>%
  select(Title)
```

These instances appear to be rather evenly split between cases that could be deemed "positive" (tough = strong) or "negative" (tough = difficult).

## Logistic Model for Topic

The NYTimes dataset comes along with a "Topic Code" (one of 27 numbers) assigned to each headline. Unfortunately, we have been unable to identify what real life topics these topic codes correspond to.

As an example, let's look at some headlines with topic code 5.

```{r, echo=F}
NYTimes %>% 
  filter(Topic.Code==5) %>%
  select(Title) %>%
  sample_frac(.1)
```

Apparently topic 5 is about labor. One more example, Topic 12:

```{r, echo=F}
NYTimes %>% 
  filter(Topic.Code==12) %>%
  select(Title) %>%
  sample_frac(.06)
```

Topic 12 seems to be crime.

We will attempt to build a logistic multinomial classifier for the topic code. This might be more useful if we knew what all the topics were, but it can still be interesting.

We need to decide what words we are going to use to build the model. If we use too many words we could overfit the data, and if we use too little we could underfit it. I will use the top 30 most common words. (Greater than 30 yielded computational issues. Also, it is possible that it may be occasionally beneficial to use less common words to discriminate among topics, but intuitively it seems useful to use words that occur many times.) The first task is to find the fifty most common words. We start with the unnested "words" data set.

```{r}
most.freq <- words$word %>%
  table()%>%
  as_tibble() %>%
  arrange(desc(n)) %>%
  slice(1:30)
model.words <- most.freq$.
```

Now we need to create a feature of each row that gives the number of times each word appears in the headline.

```{r, echo=F}
model.data = NYTimes
for(i in 1:length(model.words)){
  feature.word = model.words[i]
  for(j in 1:dim(NYTimes)[1]){
    model.data[[feature.word]][j]=str_count(NYTimes$Title[j], 
                                        or(START, SPC) %R% feature.word %R% or(END, SPC))
  }
}
```

Now we train the multinomial logistic classifier.

```{r}
model <- multinom(as.factor(Topic.Code) ~ . - Article_ID - Date - Title - Subject, data = model.data)
```

Now let's check how the model does on the training set. 

```{r, echo=F}
mean(predict(model, newdata = model.data)==model.data$Topic.Code)
```

Keeping in mind this is still a training set, not a test set, this sounds promising, considering there are 27 topics to choose from and the model. However, things change when we look at what the predictions actually are:

```{r, echo=F}
table(predict(model, newdata = model.data))
```

The model nearly always predicts topic 19, which is the most common topic in the data set, so it is basically useless. This indicates we need a more sophisticated way of measuring accuracy, and a more sophisticated way of estimating the model parameters, if not an entirely new model. These questions are beyond the scope of the current project.