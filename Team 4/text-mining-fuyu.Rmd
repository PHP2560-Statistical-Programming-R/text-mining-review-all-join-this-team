---
title: "Text Analyis"
output: github_document
---




# Your mission

Perform text analysis.

## Okay, I need more information

Perform sentiment analysis or topic modeling using text analysis methods as demonstrated in the pre-class work and in the readings.

## Okay, I need even more information.

Do the above. Can't think of a data source?

- `gutenbergr`
- `AssociatedPress` from the `topicmodels` package
- `NYTimes` or `USCongress` from the `RTextTools` package
- Harry Potter Complete 7 Books text
- [State of the Union speeches](https://pradeepadhokshaja.wordpress.com/2017/03/31/scraping-the-web-for-presdential-inaugural-addresses-using-rvest/)
- Scrape tweets using [`twitteR`](https://www.credera.com/blog/business-intelligence/twitter-analytics-using-r-part-1-extract-tweets/)

Analyze the text for sentiment OR topic. **You do not need to do both**. The datacamp courses and [Tidy Text Mining with R](http://tidytextmining.com/) are good starting points for templates to perform this type of analysis, but feel free to *expand beyond these examples*.

# Timelines and Task


We will spend the next 2 weeks working on analyzing textual data in R. You will do the following:

- Start with some text based data.
- Clean data and prepare it for analysis
- Ask questions about the data
- Answer these questions with the data using tables and graphics
- Each group member must have their own unique question that they code the answer for. 

```{r}


library(stringr)

library(rebus)

library("tidyverse")

library("lubridate")

```

```{r}
#split date and time
ivanka_trump_twitter <- read_csv("~/Documents/ivanka_trump_twitter.csv")
bday <- str_split(ivanka_trump_twitter$created_at,pattern=" ",simplify=TRUE)

names(bday)[names(bday)=="1"] <- "date"
names(bday)[names(bday)=="2"] <- "time"

#swich the format of date to someting R can read
date <- bday[,1]
date <- as.Date(date,"%m/%d/%y")

time <- bday[,2]

#separate year, month and day
pattern1 <- capture(one_or_more(DGT))  %R% "-" %R% capture(one_or_more(DGT)) %R% "-" %R% capture(DGT %R% DGT)
date1 <- str_match(date,pattern1)
colnames(date1) <- c("date","year","month","day")


```

```{r}
# compose the useful data into a new data frame
library(dplyr)
other_col <- ivanka_trump_twitter %>% select(c(source,text,id_str,is_retweet))
data <- cbind(other_col,date1,time)

```

```{r}
# 
n_tweet <- data %>%
  group_by(year,month) %>%
  mutate(timestamp=ymd(date))

ggplot(n_tweet,aes(x = timestamp,fill=year)) +
  geom_histogram(position = "identity",bins =20,show.legend=FALSE) +
  xlab("")
```

```{r}

# function that cleans data
hashgrep <- function(text) {
  hg <- function(text) {
    result <- ""
    while(text != result) {
      result <- text
      text <- gsub("#[[:alpha:]]+\\K([[:upper:]]+)", " \\1", text, perl = TRUE)
    }
    return(text)
  }
  unname(sapply(text, hg))
}

cleanposts <- function(text) {
  clean_texts <- text %>%
    gsub("<.*>", "", .) %>% # remove emojis
    gsub("&amp;", "", .) %>% # remove &
    gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", .) %>% # remove retweet entities
    gsub("@\\w+", "", .) %>% # remove at people
    hashgrep %>%
    gsub("[[:punct:]]", "", .) %>% # remove punctuation
    gsub("[[:digit:]]", "", .) %>% # remove digits
    gsub("http\\w+", "", .) %>% # remove html links
    iconv(from = "latin1", to = "ASCII", sub="") %>% # remove emoji and bizarre signs
    gsub("[ \t]{2,}", " ", .) %>% # remove unnecessary spaces
    gsub("^\\s+|\\s+$", "", .) %>% # remove unnecessary spaces
    tolower
  return(clean_texts)
}
```

```{r}
library(dplyr)
text_clean <- data %>% 
  select(text) %>%
  mutate(text_clean = cleanposts(text))
#remove stop words like "the","of","to"

library(tidytext)

tidy_text <- text_clean %>%
  mutate(linenumber=row_number())%>%
  unnest_tokens(word,text_clean) %>%
  anti_join(stop_words)

data_clean <- cbind(text_clean,data[,3:9],data[,1])
data_clean <- as_tibble(data_clean)
data(stop_words)
data_tidy <- data_clean %>%
  select(-text) %>%
  mutate(linenumber = row_number()) %>%
  unnest_tokens(word,text_clean)%>%
  anti_join(stop_words)

#the most used words
library(ggplot2)
tidy_text %>%
  count(word,sort=TRUE) %>%
  filter(n>500)%>%
  mutate(word=reorder(word,n))%>%
  ggplot(aes(word,n,fill=n))+
  geom_col()+
  xlab(NULL)+
  coord_flip()
```

```{r}

names(data_clean)
#words frequency by month


library(wordcloud)
library(dplyr)
frequency <- data_tidy %>%
  group_by(year,word) %>%
  count(year,word) %>%
  mutate(proportion = n / sum(n)) %>%
#how to facet_wrap by year???
  
  with(wordcloud(word,n,max.words =100,colors=brewer.pal(8, "Dark2")))
```

```{r}
#sentiment analysis
library(tidytext)
library(tidyr)
text_s <- data_tidy %>%
  inner_join(get_sentiments("bing")) %>%
  count(year,index = linenumber %/% 100, sentiment) %>%
  spread(sentiment,n,fill=0) %>%
  mutate(sentiment = positive - negative)

library(ggplot2)
ggplot(text_s, aes(index,sentiment,fill=year)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~year,ncol=3,scales = "free_x")

```

```{r}
# Most common positive and negative words
bing_word_counts <- data_tidy %>%
  inner_join(get_sentiments("bing")) %>%
  count(word,sentiment,sort = TRUE) %>%
  ungroup()
bing_word_counts
 
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10)%>%
  ungroup %>%
  mutate(word=reorder(word,n))%>%
  ggplot(aes(word,n,fill=sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment,scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()




```


```{r}
# Changes in word use
library(lubridate)

word_by_time <- data_tidy %>%
  mutate(timestamp=ymd(date))%>%
  mutate(time_floor = floor_date(timestamp,unit = "1 month")) %>%
  count(time_floor,word) %>%
  ungroup() %>%
  group_by(time_floor)%>%
  mutate(time_total = sum(n)) %>%
  group_by(word)%>%
  mutate(word_total = sum(n)) %>%
  ungroup() %>%
  rename(count = n) %>%
  filter(word_total > 400)

as_tibble(word_by_time)
nested_data <- word_by_time %>%
  nest(-word)

library(purrr)
nested_models <- nested_data %>%
  mutate(models = map(data,~glm(cbind(count,time_total) ~ time_floor, ., family = "binomial")))

library(broom)
slopes <- nested_models %>%
  unnest(map(models,tidy)) %>%
  filter(term == "time_floor") %>%
  mutate(adjusted.p.value = p.adjust(p.value))

top_slopes <- slopes %>%
  filter(adjusted.p.value < 0.1)

word_by_time %>% 
  inner_join(top_slopes,by="word") %>%
  ggplot(aes(time_floor,count/time_total,color=word),show.legend = FALSE) +
  geom_line(size = 0.8) +
  labs(x = NULL, y = "Word Frequency")
```

```{r}
#retweet
total <- data_tidy %>%
  group_by(id_str) %>%
  filter(is_retweet == TRUE) %>%
  summarise(rts = n()) %>%
  summarise(total_rts = sum(rts))
total

word_by_rts <- data_tidy %>%
   filter(is_retweet == TRUE)%>%
  group_by(id_str,word) %>%
  summarise(rt = first(is_retweet)) %>%
  ungroup%>%
  group_by((word)) %>%
  summarise(retweets = max(rt),uses=n()) %>%
  filter(retweets != 0) %>%
  ungroup()


word_by_rts %>%
  filter(uses >= 10) %>%
  arrange(desc(retweets))

word_by_rts
colnames(word_by_rts)<-c("word","retweets","uses")
word_by_rts %>%
  filter(uses >= 10) %>%
  top_n(10,retweets) %>%
  arrange(retweets) %>%
  ungroup %>%
  mutate(words = factor(word,unique(word)))%>%
  ungroup() %>%
  ggplot(aes(words, retweets,fill = "sky")) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(x = NULL, 
       y = "Max # of retweets for tweets containing each word")


```
