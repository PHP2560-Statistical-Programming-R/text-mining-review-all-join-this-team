---
title: "Ted Talk Analysis"
author: "Blain Morin"
date: "October 18, 2017"
output: pdf_document
---

```{r}
### Load required packages:
library(stringr)
library(tidyverse)
library(tidytext)
library(lubridate)
library(ggrepel)
library(rebus)
```

```{r}
### Read in metadata:
ted_main <- read_csv("data/ted_main.csv")

### Read in lecture transcripts:
transcripts <- read_csv("data/transcripts.csv")

### Fix url formatting so the two data sets match:
transcripts$url = str_replace_all(transcripts$url, pattern = "\r", replacement = "")
```

```{r}
### Combine data sets for sentiment analysis:
combined = inner_join(ted_main, transcripts, by = "url")
```

If we look at the transcripts, we can see that presentation events such as "applause" or "music starts" is included in between parenthesis. Since these word have sentiment values in the lexicons we must remove them.

```{r}
### Remove words that are not part of the talks:

for (i in 1:nrow(combined)){
  combined[i, "transcript"] = 
    str_replace_all(
      combined[i, "transcript"],
      pattern = "\\([^()]+\\)", 
      " "
    )
}
  
```


```{r}
### Change to token format for sentiment analysis:
transcripts_clean = combined %>% unnest_tokens(output = word, input = transcript)



### Add a total words per talk column:
transcripts_clean = transcripts_clean %>%
  group_by(name) %>%
  mutate(wordcount = n()) %>%
  ungroup()
```


## Which talks had the highest proportion of positive words?

```{r}
### Join with Bing lexicon:
transcripts_bing = transcripts_clean %>%
  inner_join(get_sentiments("bing"))
  
### Next, count the number of positive words in each talk
### Then, divide by the total words to obtain the proportion
### Then, output the top 20 most positive talks

transcripts_bing %>%
  filter(sentiment == "positive") %>%
  count(name, sentiment, wordcount) %>%
  mutate(positive_p = n / wordcount) %>%
  arrange(desc(positive_p)) %>%
  top_n(20)
```


#### These results are unexpected. We can see that the most positive reult contains just 25 words. Let's rerun the code with a filter that removes the low word count performances. 


```{r}
### First find a good cutoff point for word count. 
### Start by dividing data into 2.5% quantiles:

temp = transcripts_bing %>% 
  select(wordcount) %>%
  unlist() %>%
  quantile(probs = seq(0, 1, .025))

temp

tempplot = transcripts_bing %>%
  group_by(name) %>%
  arrange(wordcount) %>%
  filter(wordcount <= 992) %>%
  ggplot(aes(y = wordcount, x = seq_along(wordcount))) + 
      geom_smooth() +
      ggtitle("Determine a Word Count Cutoff")+
      xlab("Performances") +
      ylab("Word Count") +
      theme_minimal()

tempplot2 = tempplot + 
  geom_hline(yintercept = 510, color = "red", size = 1.5) +
  geom_text(aes(10000, 560, label = "count filter = 510"), color = "red")

tempplot2

```



```{r}

### Most artist performances are trimmed by filtering the data for word counts > 510

transcripts_clean %>% 
  group_by(name) %>%
  filter(wordcount > 510) %>%
  summarise(words = mean(wordcount)) %>%
  arrange(words)

```

```{r}
transcripts_bing %>%
  filter(sentiment == "positive") %>%
  filter(wordcount > 510) %>%
  count(name, sentiment, wordcount) %>%
  mutate(positive_p = n / wordcount) %>%
  arrange(desc(positive_p)) %>%
  top_n(20)

```

Which words contributed the most to sentiment scores?

```{r}
###Count by word and sentiment
###Then, group by sentiment
###Then, plot the top 20 words for each sentiment

transcripts_bing %>%
    count(word, sentiment) %>%
    group_by(sentiment) %>%
    top_n(20) %>%
    ungroup() %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(x = word, y= n, fill = sentiment)) +
    geom_col(show.legend = FALSE) +
    ylab("count") +
    facet_wrap(~ sentiment, scales = "free") +
    coord_flip() +
    ggtitle("Top 20 Words That Contributed to Sentiment")

```

What was the top sentiment?

```{r}
### Join with NRC Lexicon:
transcripts_nrc = transcripts_clean %>%
  inner_join(get_sentiments("nrc")) 

### Count by word and sentiment
### Group by sentiment
### Take the top 20 words for each sentiment
### Set up the plot with aes()

transcripts_nrc %>%
    count(word, sentiment) %>%
    group_by(sentiment) %>%
    top_n(5) %>%
    ungroup() %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(x = word, y= n, fill = sentiment)) +
    geom_col(show.legend = FALSE) +
    ylab("count") +
    facet_wrap(~ sentiment, scales = "free") +
    coord_flip() +
    scale_y_continuous(breaks = seq(0, 4000, 2000))

```



Did sentiments change over time?

```{r}

### Convert Unix time to a date:

transcripts_bing = transcripts_bing %>%
  mutate(year = year(as.Date(as.POSIXct(transcripts_bing$film_date, origin="1970-01-01"))))


```

```{r}
###Group by year and count the total sentiment words
bing_by_year <- transcripts_bing %>%
    group_by(year) %>%
    mutate(total_words = n()) %>%
    ungroup() 

###Create variable "p" 
bing_by_year %>%
    count(year, sentiment, total_words) %>%
    mutate(p = n / total_words) %>%
    ggplot(aes(x = year, y = p, color = sentiment)) +
    geom_line(size = 1.5) +
    geom_smooth(method = "lm", se = FALSE, lty = 3) +
    expand_limits(y = 0) +
    xlab("Year") +
    ylab("Proportion of Sentiment Word")

```

```{r}
### Want to add top tag per year
### First extract tags
### Then find top tag per year 

tagsdata = bing_by_year %>%
  distinct(title, tags, year)%>%
  unnest_tokens(output = tag, input = tags) %>%
  group_by(year, tag) %>%
  count() %>%
  ungroup() %>%
  group_by(year) %>%
  slice(which.max(n)) %>%
  select(year, tag)

bing_by_year = bing_by_year %>% inner_join(tagsdata) %>% rename(yeartoptag = tag)


bing_by_year %>%
    count(year, sentiment, total_words, yeartoptag) %>%
    mutate(p = n / total_words) %>%
    ggplot(aes(x = year, y = p, color = sentiment)) +
    geom_line(size = 1.5) +
    geom_smooth(method = "lm", se = FALSE, lty = 3) +
    expand_limits(y = 0) +
    xlab("Year") +
    ylab("Proportion of Sentiment Words") +
    geom_text(aes(x = year, y = .5, label = yeartoptag), angle = 90, color = "hotpink4")
```
